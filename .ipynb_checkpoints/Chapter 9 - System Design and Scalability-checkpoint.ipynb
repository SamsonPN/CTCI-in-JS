{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Stock Data\n",
    "1. Scope:\n",
    "    - have data on stock prices (open, close, high, low)\n",
    "    - have 1000 client apps accessing this data\n",
    "2. Assumptions:\n",
    "    - data is going to be read-only. don't want client apps to change the prices\n",
    "    - they might be requesting the data only every couple of hours since i assume stock prices don't change that much and they are asking for end-of-day stock price info\n",
    "3. Major Components:\n",
    "    - database to contain all of this info\n",
    "        - probably going to use a SQL-based DBMS like MySQL for the job. since it's mostly going to be read-only queries involved, don't need the DBMS to have complicated queries or actions\n",
    "    - going to have some sort of API to handle all client requests and to query database\n",
    "        - going to use node.js to create it since you can make clusters of the instance if traffic is heavy but it is simple enough for what we're doing\n",
    "4. Key Issues:\n",
    "    - if we just run one instance of the node.js api then it can be a big bottleneck if all of the clients make the requests within a small time period\n",
    "    - if we only have one database with all the info, then it could also be a source of a bottleneck if there are multiple queries going on\n",
    "5. Redesign:\n",
    "    - we can scale the api by using node.js clusters which will have clusters that can divide the traffic and handle all of the requests simultaneously. decent solution if we make use of a multi-core system that can do this\n",
    "    - we could have clones of the database that the clusters could access. it will also divide up the queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book Solutions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Social Network\n",
    "\n",
    "1. Scope:\n",
    "    - what features should this social network have? \n",
    "        - profiles, connections, messages, etc\n",
    "    - who is the target audience?\n",
    "        - linkedin and facebook cater to different user bases.\n",
    "        - one is for professionals looking for jobs or posting up job offers\n",
    "        - the other is a way for families, friends, acquaintances to connect\n",
    "        - identifying the user audience is important to determine what features it should have\n",
    "2. Assumptions:\n",
    "    - going to assume a generic facebook-like social network\n",
    "    - going to have a user profile, friends list, etc\n",
    "    - able to message between user and their friends either 1-1 or group chat\n",
    "    - going to have some sort of a timeline for user's to create posts and share them\n",
    "3. Major components:\n",
    "    - user profile\n",
    "    - friends list\n",
    "    - messaging system\n",
    "    - timeline\n",
    "    - data structure to use: a graph\n",
    "        - a network itself is like a big graph so it's a no-brainer for the social network to be one as well\n",
    "        - the social network itself is going to be a large graph with each of its nodes acting as a feature\n",
    "            - so one node for profiles\n",
    "            - one node for messaging system\n",
    "            - one node for timelines etc\n",
    "        - the user profiles themselves are graphs with each node being assign a different function\n",
    "            - one node for mutuals\n",
    "            - one node for user's information\n",
    "            - another node for posts\n",
    "            - and one for any messages\n",
    "4. Key issues:\n",
    "    - might have redundant information in each graph and will be pretty tedious to update them\n",
    "    - so we have to update the messages in the user profile graph and in the messaging system node in the social network graph\n",
    "5. Redesign:\n",
    "    - should just keep that information in one place to make it more efficient to update\n",
    "    - so all messages should be kept in the messaging system node and the user's messaging node has a reference to all their conversations\n",
    "***\n",
    "* for the shortest path between two users based on their mutuals:\n",
    "    1. should be using a breadth-first search to find the shortest path\n",
    "    2. essentially you run a double bfs until you land on a mutual user between the two\n",
    "        - so you run bfs from user1 --> user2\n",
    "        - and you simultaneously rn one from user2 --> user1\n",
    "        - if the algorithm lands on the same mutual or on one that the other already looked through, then you have found the shortest path\n",
    "    3. the way the data structure is set up for the social network as a big graph will help tremendously in this b/c you will have a list of friends/mutuals to work with for the double-bfs method\n",
    "* a possible optimization would be to first look through all the connections of user1 and check if they are also user 2's connections\n",
    "    - if there is 1 match, then that mutual is the shortest path between the two users and the double-bfs method is not necessary\n",
    "    - it would probably be faster to do this than jumping straight into double-bfs b/c if both of the users have huge amounts of connections, like 5k+, then that is a lot of mutuals to go through"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book Solutions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Web Crawler\n",
    "\n",
    "1. Scope:\n",
    "    - what are the infinite loops in regards to?\n",
    "    - is there a possibility for infinite loops when crawling within a domain's contents or are we talking about infinite loops when crawling through the entire web?\n",
    "        - the answer to this could change the way the algorithm is designed\n",
    "2. Assumptions:\n",
    "    - assume infinite looping in regards to pages within a site so mypage/about vs mypage/faq, etc\n",
    "    - assume site is a multi-page application rather than a single-page one\n",
    "3. Major Components:\n",
    "    - could have a hash table that takes in the URL of the page currently being crawled\n",
    "    - if the crawler enters a site that it's already seen before, it will abort the operation and move onto other pages or sites\n",
    "    - within that hash table, each site will also have info on the contents of the site\n",
    "    - should have another hash table with some info on common html elements like head or body tags to make a comparison in case the URL check fails\n",
    "4. Key issues:\n",
    "    - for URLs, there could be sites that have similar URLs but pretty much the same content and that could be accidentally crawled infinitely\n",
    "    - it also doesn't take into account the same domains/subdomains for a page\n",
    "5. Redesign:\n",
    "    - the hash table should also add in info on domains/subdomains that have similar extensions, etc\n",
    "    - especially for sites that use QUERIES at the end of the URLs which would have different URLs but pretty much the same content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book Solutions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 Duplicate URLs\n",
    "\n",
    "1. Scope:\n",
    "    - what should we do with the duplicate URLs? should we just notify someone that there is a duplicate URL or should it be removed?\n",
    "    - how are these URLs gathered and stored in the first place? is there a list of these URLs or do we have to crawl for them?\n",
    "    - is there a distinction between domains and subdomains?\n",
    "        - do maps.google.com and google.com count as duplicates?\n",
    "2. Assumptions:\n",
    "    - assume that the whole URL has to be unique\n",
    "        - so google.com/results, google.com, and maps.google.com are three unique results\n",
    "    - also assume that the URLs are stored in a list of some sort, like an array\n",
    "3. Algorithm:\n",
    "    1. want to have iterate through the entire list of URLs and add them to a hash table\n",
    "    2. while iterating, check to see if the URL is already in the hash table. if it is, then we can notify that it is a duplicate and move on. \n",
    "        - this will essentially take O(n) space where n = # of URLs. in this case, n = 10 billion URLs\n",
    "        - it should only take 1 pass through this list of URLs to notify and find any duplicates b/c as you pass through the list, you only keep track of unique URLs in the hash table and any duplicates will immediately be found\n",
    "        - it should only take 1 machine to do\n",
    "4. Key issues:\n",
    "    - if we wanted to scale this further to more than 10 billion URLs, we might not have enough space to fit all the URLs onto one machine\n",
    "5. Redesign:\n",
    "    - we could split up the list of URLs into multiple parts and do the same algorithm on multiple machines\n",
    "    - then once we are done, we can compare all the URLs in each hash table to see if there are duplicates between the hash tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book Solutions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5 Cache\n",
    "\n",
    "1. Scope:\n",
    "    - does processSearch(query) get results from a subset of the 100 machines available or is it from an entirely different cluster of machines?\n",
    "    - is processSearch(query) expensive in time or in space?\n",
    "    - how recent should the search results be? should it only be the most recent 1k, 100k, 1 million, etc results?\n",
    "2. Assumptions:\n",
    "    - assume a different set of machines handles the search than the current 100 machines\n",
    "    - assume processSearch() is a time-expensive thing\n",
    "    - assume data from machines are sent back to the web server\n",
    "    - assume most recent 100k results\n",
    "3. Major components:\n",
    "    - web server that assigns a query to a random machine\n",
    "    - 100 machines that are randomly selected for the results\n",
    "    - the separate cluster of machines that handle the actual search\n",
    "    - the client that sends the query to the web server\n",
    "    - a hash table containing the most recent results\n",
    "        - key = the query which is a string\n",
    "        - value = the result sent back from processSearch()\n",
    "        - on the web server, it will receive the query from the client and check it against this hash table\n",
    "        - if it is present in the table, then return the result. else, call on one of the machines to call processSearch()\n",
    "        - and if we have reached the threshold of 100k results in the table, the remove the oldest ~10k or so results in the table to make room for more\n",
    "4. Key Issues:\n",
    "    - will require some space for the hash table if there are lots of results to be cached\n",
    "    - removal of the oldest 10k results in a hash table might be difficult b/c results are not ordered in any way\n",
    "    - and it might be costly to remove that many results\n",
    "5. Redesign:\n",
    "    - use of a linked list with each node being a hash table\n",
    "    - linked list will have a head pointer and a tail pointer and the hash table has a capacity of ~10k results\n",
    "    - so once we reach the 10k results, the linked list will add another node at the head that contains another hash table\n",
    "    - and the node at the tail will be removed\n",
    "    - searching for the result will not be expensive b/c there will be at most 10 nodes for the most recent 100k results\n",
    "    - and removing the oldest 10k results is as simple as updating the tail pointer to reference the node prior to the last one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book Solutions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.6 Sales Rank\n",
    "\n",
    "1. Scope:\n",
    "    - how are these products stored? is there just one general table called 'Products' are are they already stored by categories such as Sports, Home, Kitche, etc\n",
    "    - does each product have a list of categories that describe what it is meant for? like if a product was part of Kitchen, Home, Utensil or something\n",
    "    - is there a master list of all available categories in the eCommerce site?\n",
    "    - are the categories stagnant or do they add new categories?\n",
    "2. Assumptions:\n",
    "    - assume that all products are stored in one table called 'Products' with a list of categories describing what the product is menat for\n",
    "    - assume that there is some sort of master list with all the categories of their products available\n",
    "    - assume that the master list is stagnant and no new categories are added into it\n",
    "3. Major Components:\n",
    "    - list of all products on the site\n",
    "    - list of all categories of products\n",
    "    - the system would work like this:\n",
    "        - it would iterate through every product in the Products table\n",
    "        - and it would then create an array for each category that the products are attached to, i.e. create a Sports array and a Home array\n",
    "        - then it would add the product to this array\n",
    "        - once all products are accounted for, each of these arrays will then be sorted by # of products sold and to determine the rankings\n",
    "        - this would also be done for the Overall category as well\n",
    "        - and the system would only be doing this every hour or so. could be more frequent if the eCommerce site usually has a high amount of traffic. so it could be done every 15 minutes or so\n",
    "4. Key issues:\n",
    "    - if there is a huge amount of products available, then iterating through each one then sorting them by categories will be an expensive task time-wise\n",
    "    - not to mention it would also require a lot of space too to store all the products in arrays for each category\n",
    "5. Redesign:\n",
    "    - would be able to cache some of the results of the previous rankings and make adjustments if necessary.\n",
    "    - this would be good if some products are not as frequently sold as others so their rankings are quite stagnant whereas the more frequently sold items would need to be constantly sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book Solutions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.7 Personal Financial Manager\n",
    "\n",
    "1. Scope:\n",
    "    - what types of features exactly should be available?\n",
    "        - when they say 'Make recommendations' do they mean make recommendations on spending habits? on stocks? what exactly is it?\n",
    "    - how do we connect to the bank accounts of these users and how often should their data be requested? what kind of data do we need from them?\n",
    "    - what types of habits are we looking for in terms of spending and how exactly do we categorize it?\n",
    "2. Assumptions:\n",
    "    - assume that it will track purchases that are provided by bank statements and make recommendations on how to reduce spending and budget for other things\n",
    "    - should probably categorize by essential and non-essential purchases. should probably give the user the ability to categorize their purchases as well since some users prioritize some purchases over others, e.g. streamers prioritize faster internet speeds whereas the avg person will just get an average plan\n",
    "3. Major Components:\n",
    "    - some sort of API that will help connect us to people's bank account information in regards to purchases. Chase bank has its own API for that sort of thing so might have to use multiple APIs to get access to all the popular banks\n",
    "    - will have a database of some kind to store information on purchases for up to a year or some other predefined amount of time and each purchase will be categorized by essential or non-essential or some other categories as well\n",
    "    - will need some sort of analytics implemented to show spending in general to the user\n",
    "    - the API should be updated every day or every couple of days to show trends in spending habits so far\n",
    "4. Key issues:\n",
    "    - not a lot of the data that we fetch needs to be put into a database if we can already request it from bank APIs so having it would just be a waste of space\n",
    "    - not to mention users will have variable purchasing habits so it is hard to tell whether to request the data frequently or infrequently from these APIs\n",
    "5. Redesign:\n",
    "    - the user should get to decide how often their purchases should be fetched. some users might like to look at this info on a monthly basis while others might want to look at it daily. this will help reduce the amount of calls to these APIs\n",
    "    - purchases should not be kept in a database at all and instead their spending habits should be saved. any time we want to look at purchases, we ask the API. and any time users want to look at prior spending habits, we already have it saved in our database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book Solutions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.8 Pastebin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book Solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Javascript (Node.js)",
   "language": "javascript",
   "name": "javascript"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "application/javascript",
   "name": "javascript",
   "version": "13.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
